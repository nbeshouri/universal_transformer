wandb_version: 1
tokenizer:
  value: 'en_core_web_md'
vectors:
  value: 'en_core_web_md'
dataset:
  value: 'babi'
model:
  value: 'universal_transformer'
optimizer:
  value: 'adam'
epochs:
  value: 10
batch_size:
  value: 128
lr:
  value: 0.0001
log:
  value: null
learning_rate_decay_schedule:
  value: null
log_freq:
  value: null
checkpoint_metric:
  value: 'accuracy'
dataset.debug:
  value: True
dataset.task:
  value: 3
dataset.version:
  value: '10k'
model.nhead:
  value: 2
model.max_steps:
  value: 6
model.halting_threshold:
  value: 0.9
dynamic_halting_loss_weight:
  value: 0.001
model.transition_hidden_size:
  value: null  # Meaning same as embedding size.
