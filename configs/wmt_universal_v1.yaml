wandb_version: 1
tokenizer:
  value: 'hugging_face_word_level'
output_tokenizer:
  value: 'hugging_face_word_level'
vectors:
  value: 'random'
output_vectors:
  value: 'random'
vectors.vector_size:
  value: 90
dataset:
  value: 'wmt'
model:
  value: 'universal_transformer'
optimizer:
  value: 'adam'
epochs:
  value: 2
batch_size:
  value: 128
lr:
  value: 0.0001
log:
  value: null
learning_rate_decay_schedule:
  value: null
log_freq:
  value: 100
dataset.debug:
  value: False
model.nhead:
  value: 3
model.max_steps:
  value: 3
tokenizer.fixed_length:
  value: 100
model.halting_threshold:
  value: null
dynamic_halting_loss_weight:
  value: null
